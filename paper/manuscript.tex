\documentclass[12pt]{article}
\usepackage{amsmath}
\usepackage{times}
\usepackage{graphicx}
\usepackage{color}
\usepackage{multirow}
%
\usepackage[authoryear]{natbib}
%
\usepackage{rotating}
\usepackage{bbm}
\usepackage{latexsym}
%\DeclareGraphicsExtensions{.eps,.png}

%%% margins 
\textheight 23.4cm
\textwidth 14.65cm
\oddsidemargin 0.375in
\evensidemargin 0.375in
\topmargin  -0.55in
%
\renewcommand{\baselinestretch}{2}
%
\interfootnotelinepenalty=10000
%
\renewcommand{\thesubsubsection}{\arabic{section}.\arabic{subsubsection}}
\newcommand{\myparagraph}[1]{\ \\{\em #1}.\ \ }
\newcommand{\citealtt}[1]{\citeauthor{#1},\citeyear{#1}}
\newcommand{\myycite}[1]{\citep{#1}}

% Different font in captions
\newcommand{\captionfonts}{\normalsize}

\makeatletter  
\long\def\@makecaption#1#2{%
  \renewcommand{\baselinestretch}{1}% EH: stop captions running off the bottom of the page
  \vskip\abovecaptionskip
  \sbox\@tempboxa{{\captionfonts #1: #2}}%
  \ifdim \wd\@tempboxa >\hsize
    {\captionfonts #1: #2\par}
  \else
    \hbox to\hsize{\hfil\box\@tempboxa\hfil}%
  \fi
  \vskip\belowcaptionskip}
\makeatother   
%%%%%

\newcommand{\ie}{\emph{i.e., }}
\newcommand{\eg}{\emph{e.g., }}
%% \newcommand{\cf}{\emph{cf. }}
%% \newcommand{\etc}{\emph{etc.}}
%% \newcommand{\wrt}{w.r.t. }

\usepackage{nameref}
\newcommand{\fig}[1]{Figure \ref{fig:#1}}
\newcommand{\figlabel}[1]{\label{fig:#1}}
\newcommand{\eqn}[1]{Equation \ref{eqn:#1}}
\newcommand{\eqnlabel}[1]{\label{eqn:#1}}
\newcommand{\scn}[1]{\textsc{\nameref{scn:#1}}}
\newcommand{\scnlabel}[1]{\label{scn:#1}}

\renewcommand{\thefootnote}{\normalsize \arabic{footnote}} 	

\begin{document}
\hspace{13.9cm}1

\ \vspace{20mm}\\

{\LARGE Heterogeneity increases information transmission of neuronal populations}

\ \\
{\bf \large Eric G. Hunsberger$^{\displaystyle 1}$, Matthew Scott$^{\displaystyle 2}$, Chris Eliasmith$^{\displaystyle 1}$}\\
{$^{\displaystyle 1}$Centre for Theoretical Neuroscience, University of Waterloo, Waterloo, Ontario, Canada.}\\
{$^{\displaystyle 2}$Department of Applied Mathematics, University of Waterloo, Waterloo, Ontario, Canada.}\\
%

%\ \\[-2mm]
{\bf Keywords:} Heterogeneity; stochastic resonance; population coding

\thispagestyle{empty}
\markboth{}{NC instructions}
%
\ \vspace{-0mm}\\
%
%Abstract
\begin{center} {\bf Abstract} \end{center}
Noise---specifically random fluctuations added to the membrane voltages of a neuronal population---has been shown to increase the amount of information a neuronal population can encode about an input signal; this phenomenon is one type of stochastic resonance. We demonstrate that heterogeneity within a population of simulated neurons---specifically the random variation of membrane bias currents across a population---produces similar benefits as additive noise in terms of increasing information encoding within the population. Furthermore, we find that both noise and heterogeneity use a common set of three mechanisms to increase information transmission: 1) they temporally desynchronize the firing of neurons within the population, 2) they decrease the response time of the population of neurons to sudden changes in the input signal, and 3) they linearize the response of a population to a stimulus. We examine each mechanism in detail, describing how these mechanisms operate in neuronal populations, and demonstrating how they operate in populations of simulated FitzHugh-Nagumo and leaky integrate-and-fire neurons. These three mechanisms work to distribute information equally across all neurons in the population, both in terms of signal timing (desynchronization and response time) and in terms of signal amplitude (linearization). Without noise or heterogeneity, all neurons in the population encode the same aspects of the input signal; adding noise or heterogeneity allows all neurons to encode \emph{complementary} aspects of the input signal, thereby preserving more information. Overall, the simulations detailed in this paper highlight the importance of heterogeneity in neuronal population coding, and demonstrate that it can be equally effective as stochastic resonance in increasing information transmission. 
%%%%%%%%%%%

\section{Introduction}
\scnlabel{introduction}

The term stochastic resonance (SR) refers to the beneficial effects of noise, specifically the phenomenon whereby a non-zero amount of stochastic fluctuation over time in the system improves some measure of performance \citep{McDonnell2009}. Since its conception, the idea of stochastic resonance has developed into a field of its own, a field which has found increasing relevance for neuroscience.

The early stochastic resonance literature dealt mainly with \emph{subthreshold} stochastic resonance \citep{Gammaitoni1998}. In a deterministic setting, systems which only respond to inputs above a certain threshold---such as neurons---cannot transmit any information about subthreshold input signals, \ie input signals which are entirely below the system's threshold. In contrast, a noisy input signal (\ie a signal with an added stochastic offset) will occasionally cross the system's threshold, and thereby allows the system to transmit more information about the input signal. Too much noise, though, causes the input signal to become lost in random fluctuations. A moderate amount of noise---not too much, and not too little---is needed to optimize information transmission by the system about the input signal \citep{Wiesenfeld1994,Longtin1998}.

A new form of stochastic resonance---called \emph{suprathreshold} stochastic resonance (SSR)---has emerged in the past decade \citep{McDonnell2009}. In neuroscience, suprathreshold SR looks at the benefits of noise to populations of nonlinear neurons receiving a common signal of which a significant portion is above the systems' firing threshold(s). This concept was first introduced by \cite{Stocks2000}, who looked at groups of binary threshold units, and found that noise can increase information transmission in such systems. Subsequent research has examined the SSR effect in populations of more sophisticated simulated neurons \citep{Stocks2001}.

Heterogeneity among neurons---the fact that not all neurons are identical, and vary across a wide range of parameters---is a topic that has largely been ignored in the stochastic resonance literature, and neuroscience literature in general \citep{Machens2010}. It is well known that there are large differences between neurons in different parts of the nervous system, and that there are many different types of neurons, but research into the role of heterogeneity within a group of neurons of the same type is limited (\cite{Eliasmith2003} is a notable exception). In the field of stochastic resonance, the effects of noise in a heterogeneous population have been examined, but generally in specific cases such as for input signals much larger than the range of heterogeneity \citep{Stocks2000}, or for very large levels of noise \citep{McDonnell2006}. Furthermore, research into stochastic resonance with heterogeneity has focused on populations of binary threshold units, which fail to capture the dynamics of actual neurons.

The purpose of this paper is to compare the benefits of noise and the benefits of heterogeneity on information transmission by a population of simulated neurons. While previous studies have looked at the effects of noise, often claiming that noise is the central mechanism of SR and SSR effects \citep{Collins1995,Stocks2001} (see \cite{McDonnell2011} for a review), we are not aware of any work that has examined the effects of heterogeneity in similar circumstances. As we show, many effects of noise can also be achieved with heterogeneity. We argue that this is at least partly because of several shared effects on neural coding. We use both the FitzHugh-Nagumo (FHN) neuron model, a common model in SR research \citep{Wiesenfeld1994,Collins1995,Longtin1998,Stocks2001}, as well as the leaky integrate-and-fire (LIF) neuron model, a typical model of cortical neurons \citep{Koch1999}. We use a common aperiodic random signal $s(t)$ to drive populations of FHN and LIF neurons with varying levels of internal noise and heterogeneity, and compare the similarity of the decoded output signal with the input signal using mutual information. Stochastic offset is modeled as zero-mean Gaussian white noise with standard deviation $\sigma_\eta$, independently and identically distributed across all neurons (see \scn{methods}). Heterogeneity was added by randomly selecting the bias voltages $b_i$ from a uniform interval of varying width. We report how noise and heterogeneity both act to increase information transmission through these populations. They achieve this using three common mechanisms---linearizing the response of a population of neurons to a stimulus, desynchronizing neuronal firing, and decreasing the reaction time of a population of neurons to a stimulus---which we examine in detail.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Results}
\scnlabel{results}

We performed numerical experiments analyzing how noise and heterogeneity affect the information transmission capacity of populations of both FitzHugh-Nagumo (FHN) neurons and leaky integrate-and-fire (LIF) neurons, to test whether heterogeneity has similar effects to noise in increasing information transmission. Specifically, trials were performed using four distinct levels of random heterogeneity across the bias voltages $b_i$, the levels being none, low, medium, and high, corresponding to 0, 0.1, 0.5, and 2 times the input signal standard deviation $\sigma_s = 0.1$ (where the highest level spans the expected range of the input signal). At each level of heterogeneity, the standard deviation $\sigma_\eta$ of a stochastic offset---applied to the voltage parameter of the neuron model---was varied logarithmically from an insignificant level ($\sigma_\eta = 10^{-9}$ for FHN neurons and $\sigma_\eta = 10^{-6}$ for LIF neurons) to levels 10 times the magnitude of the input signal ($\sigma_\eta = 10^0$), in increments of $\log_{10}(\Delta\sigma_\eta) = 0.1$. For the remainder of this paper, we will use the term noise to refer specifically to this stochastic offset. At each level of heterogeneity and noise, 25 trials were performed to achieve an accurate estimate of the mean information transmission, with each trial using a unique set of random biases and a unique set of stochastic offsets added to the membrane voltage.

Each trial consisted of presenting a population of $N = 64$ FHN or LIF neurons with 4.5 seconds of band-limited aperiodic random signal with zero mean and standard deviation $\sigma_s = 0.1$. This signal has equal power below 5 Hz and zero power above 5 Hz, where the threshold of 5 Hz was chosen because it is significantly slower (approximately $\frac{1}{5}$) of the maximum firing rate of the FHN and LIF neurons (25--30 Hz). The population was divided into 32 ``on'' neurons and 32 ``off'' neurons, where each ``on'' (``off'') neuron received a positive (negative) version of the aperiodic input signal as input. The FHN or LIF neurons were simulated using their respective differential equations, and the output was decoded using a simple summing neuron that summed and low-pass filtered the outputs of the individual neurons to obtain the population output (outputs from ``off'' neurons were multiplied by -1 before summing). Finally, the mutual information between the aperiodic input signal and the decoded output signal was calculated. The mutual information for each level of heterogeneity and noise was computed as the average mutual information from all 25 trials at those levels. See \scn{methods} for the full details of the experimental setup.

\fig{info} shows the results of these experiments. In the homogeneous populations, a strong stochastic resonance effect is present, in which the transmitted information (\ie mutual information between the input and decoded output signals) was optimized for noise levels of $\sigma_\eta \approx 3 \times 10^{-3}$ (FHN neurons) and $\sigma_\eta \approx 1 \times 10^{-2}$ (LIF neurons). These noise levels are about 30 times (FHN neurons) or 10 times (LIF neurons) smaller than the input signal standard deviation of $\sigma_s = 0.1$. This makes them small enough not to significantly degrade the input signal, while being large enough to elicit stochastic resonance effects.

\begin{figure}
  \ifx\hidefigures\undefined
    \centering
    \includegraphics[width=\textwidth]{../figures/bw/figure1_info.eps}
  \fi
  \caption{
    \textbf{Heterogeneity increases information transfer in populations of FHN and LIF neurons.} We simulated populations of 64 FHN neurons and 64 LIF neurons, varying the level of noise ($x$-axis) and heterogeneity (individual traces), and measured the mutual information between the aperiodic input signal $s(t)$ (where $\sigma_s = 0.1$) and the output signal $r(t)$ ($y$-axis). Moderate to high levels of heterogeneity resulted in an increase in information transmission at low to moderate noise levels in both FHN and LIF neurons. The increases in FHN neurons were modest, with a strong stochastic resonance effect still exhibited at all levels of heterogeneity. The increases in LIF neurons were much more pronounced, with moderate to high levels of heterogeneity surpassing the results achieved by stochastic resonance.
  }
  \figlabel{info}
\end{figure}

For both FHN and LIF neurons, heterogeneity also increases the information transmission of the population, specifically for noise levels below the optimal stochastic resonance noise level. In FHN neurons, the benefits of adding heterogeneity to neuron biases are modest (see \scn{phase}), resulting in a significant increase in information transmission, but not achieving the same levels of information transmission as stochastic resonance (\fig{info}A). For LIF neurons, the benefits of heterogeneity in neuron biases are pronounced, with moderate to high levels of heterogeneity achieving the same or even better levels of information transmission---across a wide range of noise values---as the optimal level of noise (\fig{info}B).

In the remaining experiments, we consider three mechanisms that allow both noise and heterogeneity to improve signal transmission in populations of neurons: 1) putting neurons out of phase with one another, 2) decreasing the reaction time of a population of neurons to a stimulus, and 3) linearizing the response of a population of neurons to a stimulus. Though noise and heterogeneity affect neurons differently, they both act to achieve these three goals. None of the three factors is sufficient in and of itself to explain the effects of stochastic resonance and heterogeneity on nonlinear dynamical systems, such as the populations of FHN and LIF neurons studied. Nevertheless, by examining each factor individually, we gain a deeper understanding of how both stochastic resonance and heterogeneity operate in nonlinear dynamical systems, and ultimately show that the two concepts are fundamentally connected (see \scn{discussion}).

\subsection{Phase}
\scnlabel{phase}

Consider the following: a group of perfectly homogeneous neurons with no noise, such that their dynamics are completely deterministic. Begun with the same initial conditions, they will follow exactly the same trajectory in phase space, since they all receive the same input signal and obey the same set of differential equations. Such a population of simulated neurons will exhibit perfectly synchronized firing events, and therefore the neurons contribute redundant information: any signal that could be decoded from the firing events of this population of neurons could also be decoded from the spikes of a single neuron. These neurons are perfectly in phase: at any time, they are all in the same phase of their firing cycle, and in the same position in phase space.

Adding independent noise to each neuron desynchronizes the firing events of the population over time by causing diffusion of the neuronal phases. A small random perturbation causes a slightly advanced or delayed firing of a neuron, creating a small shift in the phase of that neuron. Specifically, a firing neuron follows a stable limit cycle attractor in phase space; perturbations tangent to the limit cycle result in diffusion along the limit cycle, while perturbations perpendicular to the limit cycle rapidly return the the stable attractor \citep{Tomita1974}. Over time, this diffusion along the limit cycle results in the population of neurons having a wide distribution of phases (\fig{phase}), and the neurons fire asynchronously. 

\begin{figure}
  \ifx\hidefigures\undefined
    \centering
    \includegraphics[width=\textwidth]{../figures/bw/figure2_phase.eps}
  \fi
  \caption{
    \textbf{Noise and heterogeneity both desynchronize neurons.} To measure whether noise and heterogeneity have a desynchronizing effect on populations of neurons, we measured the standard deviation of neuron phases across the population (see \scn{methods}), where a smaller standard deviation indicates increased synchronization. Heterogeneity is seen to significantly desynchronize both FHN and LIF neurons, specifically across low to moderate levels of noise. At high levels of noise, the noise itself is sufficient to desynchronize neurons, and additional heterogeneity has no effect.
  }
  \figlabel{phase}
\end{figure}

This previous example is, of course, artificial: it requires that all neurons are \emph{exactly} the same, and start with \emph{exactly} the same initial conditions. In a more realistic scenario, one expects that small differences in neuronal parameters or small amounts of noise will cause neurons to desynchronize over a long period of time. This is not the case, because signals below the firing thresholds of a group of neurons have a synchronizing effect on the group. By stopping neuronal firing, these signals push all the neurons in the group towards their stable equilibrium points. When the signal returns to an amplitude above the firing thresholds of neurons in the group, all neurons will begin firing at approximately the same time, and will be once again in phase. Signals with higher frequency components are able to cross larger ranges of firing thresholds in quick succession, and have a stronger synchronizing effect than purely low frequency signals. Thus, more negative (positive) portions of the input signal repeatedly act to synchronize ``on'' (``off'') neurons, and groups of neurons must be able to desynchronize quickly after these synchronization events in order to remain out of phase with one another.

Similar to noise, heterogeneity acts to desynchronize neurons. Adding heterogeneity to a population significantly increases the range of distribution of neuron phases, thereby desynchronizing populations of both FHN and LIF neurons (\fig{phase}). Compared with a noiseless homogeneous population, where changes in an input signal will cross the common bias of all neurons at the same time and thereby put all neurons in phase with one another, changes in an input signal in a heterogeneous population will sequentially cross the biases of different neurons, causing some to begin firing before others. For example, an increasing input signal will cause neurons with lower biases to begin firing before those with higher biases, resulting in desynchronized firing within the population of neurons.

High frequency input signals may cross the biases of many neurons in a heterogeneous population at the same time, causing a large group of neurons to begin firing simultaneously. This effect does not have a significant synchronizing effect on LIF neurons, since the LIF tuning curves used have significantly different firing rates for different sizes of suprathreshold signal (\fig{tuning}). Specifically, a high-frequency increase in signal amplitude starts neurons firing at approximately the same time, but starts them firing at different rates, allowing them to quickly desynchronize. FHN neurons, however, all have around the same firing rate regardless of the strength of the suprathreshold signal (\fig{tuning}), and heterogeneity with higher-frequency signals has a reduced effect, since these signals cause many neurons to begin firing at once, unable to desynchronize due to their common firing rate. This discrepancy between LIF and FHN neurons results in heterogeneity having a large effect on phase in LIF neurons (\fig{phase}B), whereas the desynchronizing effect of heterogeneity in FHN neurons is much more modest (\fig{phase}A).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Response time}

Both noise and heterogeneity benefit the response time of a population of neurons, \ie how quickly the population responds to a rapidly changing signal. To measure this, we presented populations of FHN and LIF neurons with a step function and observed how well the population can follow the step function (\fig{step}). Though noise and heterogeneity both offer decreased response time, they achieve this using different mechanisms.

\begin{figure}
  \ifx\hidefigures\undefined
    \centering
    \includegraphics[width=\textwidth]{../figures/bw/figure3_step.eps}
  \fi
  \caption{
    \textbf{Noise and heterogeneity both improve the step response of neuronal populations.} We excited populations of 100 FHN neurons and 100 LIF neurons using a step function, and measured the responses of populations without noise or heterogeneity ($b_i = 0, \sigma_\eta = 0$), with noise ($b_i = 0, \sigma_\eta = 10^{-3}$), and with heterogeneity ($b_i \in [-0.01,0.01], \sigma_\eta = 0$). Each line depicts the average population response over 100 trials, and the surrounding region shows one standard deviation. FHN populations without noise or heterogeneity show a considerable delay in responding to the step function, because all neurons are close to their stable equilibrium. Both noise and heterogeneity significantly reduce this delay, and also reduce oscillations by desynchronizing neurons (\fig{phase}). LIF populations only exhibit a mild initial delay, which noise and heterogeneity both correct; in LIF neurons, the main benefit to the step response is reducing oscillations through desynchronization.
  }
  \figlabel{step}
\end{figure}

Noise increases the response time of neurons by keeping them away from their stable equilibria during low points in the input signal. By presenting a population of 100 FHN neurons with a step input function, we found that a homogeneous population with no noise showed a considerable delay between the leading edge of the input step function and the response of the population (\fig{step}). Adding noise to the population decreases the response such that it is negligible, with a high proportion of the neurons responding immediately to the leading edge of the step. Noise produces this benefit by keeping neurons away from their stable equilibria during the below-threshold portions of the input signal (\ie before the leading edge of the step), allowing them to respond quickly to above-threshold portions of the input signal (\ie after the leading edge of the step). Neurons not subject to noise, on the other hand, are brought very close to their stable equilibria during the subthreshold portion of the input signal, and it takes a mildly suprathreshold signal---as provided by the step function---a long time to bring the neuron away from this equilibrium and into the tonic firing regime.

Heterogeneity also acts to increase the response time of a population of neurons. It does not directly keep neurons away from their equilibrium points, but by spreading neuron biases across a wide range, it ensures that neurons remain in different states of activation. Upon receiving a low input signal, most neurons will be pushed towards their stable equilibria, but neurons with higher firing thresholds will be pushed there more quickly than neurons with lower thresholds. Specifically, at the rising edge of the signal, neurons with low thresholds will be less settled to their equilibria than those with high thresholds, allowing them to respond more quickly. Furthermore, the high input signal will push neurons with low thresholds away from their equilibria more quickly than those with high thresholds, again resulting in quick activation of low threshold neurons. This effect can be observed in the response of the FHN population to a step function (\fig{step}), where the heterogeneous population responds more quickly than the noiseless homogeneous population, but still less quickly than the noisy homogeneous population.

\subsection{Linearization}

Intrinsic noise makes neuron tuning curves more linear, using a mechanism similar to subthreshold stochastic resonance. For input signals below the neuron's threshold, noise operates in a manner identical to subthreshold stochastic resonance, occasionally raising the neuron's membrane voltage high enough to cross the threshold and trigger an action potential. For input signals above the neuron's threshold, noise causes the membrane voltage to occasionally fall below the threshold, slowing the firing rate slightly compared with the noise-free case. For signals well below the neuron's threshold, the noise is not sufficient to cause any spikes, and for signals well above the threshold, the noise is insufficient to stop any spikes: these portions of the tuning curve remain unchanged between the noise-free and noisy cases. Overall, the addition of noise has the effect of linearizing the response of the neuron near its firing threshold, resulting in a smoother transition from the silent to tonic firing regimes (\fig{tuning}). This result has been previously observed and extensively studied \citep{Stocks1996,Chialvo1997}.

\begin{figure}
  \ifx\hidefigures\undefined
    \centering
    \includegraphics[width=\textwidth]{../figures/bw/figure4_tuningnoisy.eps}
  \fi
  \caption{
    \textbf{Noise linearizes neuron tuning curves.} We probed FHN neurons and LIF neurons using constant inputs at various voltages, using varying levels of noise, and measured the output firing rate. Noise has the effect of making the tuning curves more linear, specifically eliminating the sharp threshold in both FHN and LIF neurons. The main mechanism is that of subthreshold stochastic resonance: noise allows subthreshold inputs to cause spikes by occasionally pushing the membrane voltage above the threshold. This occurs in both FHN and LIF neurons, allowing them to represent subthreshold signal components. In FHN neurons, small levels of noise can also cause firing rates for small suprathreshold signals to drop slightly by briefly pushing the neuron away from its firing limit cycle in state space, though interestingly this does not happen for larger noise levels. Large levels of noise in FHN neurons can cause large fluctuations which jump from one point on the limit cycle to another point much farther ahead, causing elevated firing rates. In LIF neurons, noise added to small suprathreshold signals causes the neuron to fire slightly faster, because the noise can cause the rising membrane voltage to prematurely cross the threshold, causing the spike to occasionally occur sooner than it would in the noiseless case.
  }
  \figlabel{tuning}
\end{figure}

Neuron heterogeneity has a linearizing effect similar to adding noise to a population, but unlike the noisy case, where it made sense to talk about noise linearizing an individual neuron's tuning curve, here we will discuss how heterogeneity linearizes the \emph{population's} tuning curve. The population's tuning curve is the set of points connecting the input signal strength to the decoded output. Since our model uses a simple summing decoder, the population tuning curve is equal to the sum of the individual neuron tuning curves. For a population of neurons with uniformly varied random biases, the population tuning curve is much more linear than a single neuron's tuning curve, whereas in the homogeneous case the population's tuning curve is the same as the neurons' tuning curve (\fig{tuninghetero}). This linearizing effect of heterogeneity allows for better representation of an input signal.

\begin{figure}
  \ifx\hidefigures\undefined
    \centering
    \includegraphics[width=\textwidth]{../figures/bw/figure5_tuninghetero.eps}
  \fi
  \caption{
    \textbf{Heterogeneity linearizes the response of a neuronal population to a stimulus.} Panel A shows the tuning curves for a heterogeneous population of $N = 9$ neurons, where one neuron has been chosen to have a bias of $-0.15$ and the other neurons have random biases uniformly sampled from the interval $[-0.15,0.15]$. Panel B shows how the population response (heavy solid line) is much closer to a linear response (heavy dot-dashed line) than a single LIF tuning curve (heavy dashed line), even though the heterogeneous biases are far from evenly spaced. The light traces indicate how the population response is the average of the individual neuron tuning curves, where each colored curve is a scaled version of the tuning curves in panel A (with scaling factor $1/N$), and the population response is the sum of these curves. Using a larger population of neurons (\ie increasing $N$) results, on average, in a more linear population response.
  }
  \figlabel{tuninghetero}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Discussion}
\scnlabel{discussion}

Our results show that noise and heterogeneity in populations of neurons can improve the ability of a neuronal population to encode and transmit a signal. Both noise and heterogeneity can be understood as using similar mechanisms to achieve this increase in information transmission, specifically by 1) desynchronizing the neurons within the population, 2) improving the reaction time of the population to a rapidly changing signal, and 3) linearizing the response of the population. Consequently, both noise and heterogeneity may be important mechanisms for optimizing information flow in neural systems.

To characterize the benefits of both noise and heterogeneity on signal transmission in populations of neurons, we can separate them into two categories: \emph{temporal} properties of neurons, and \emph{rate} properties of neurons. The temporal category is concerned with the desynchronizing and reaction time effects of noise and heterogeneity, and the rate category is concerned with the linearizing property of both noise and heterogeneity on a neuronal population.

The temporal benefits of noise and heterogeneity can only be observed in spiking neuron models, such as the FHN and LIF models examined in this paper. The output of these models depends not only on the the output firing rate for a given input signal, as determined by the neuron tuning curve, but also by the timing of individual spike events. When nonlinear dynamical models of neurons are examined in a stochastic resonance simulation, as in \cite{Stocks2001}, the results are a combination of the temporally-based desynchronizing and response-time effects of noise, as well as the rate-based linearizing effects of noise.

The rate view of neurons treats them as being no more than input-output (or tuning) curves. That is, all temporal properties are ignored, and a neuron is reduced to a nonlinear function which turns an input signal into an output signal. Suprathreshold stochastic resonance effects observed in binary threshold units deal only with the rate benefits of noise \citep{Stocks2000,Stocks2001a,McDonnell2006}. This is because binary threshold units have no temporal properties---they are either on or off depending on the current input signal and noise---and cannot be affected by the temporal benefits of noise. The linearization effects of noise on nonlinear systems have been previously studied (see \cite{Stocks1996} for a review). We have observed that heterogeneity can also achieve these linearization effects across a population of neurons, which has previously been reported by \cite{Eliasmith2003}.

One important difference between the LIF and FHN neuron models is that the LIF model is a type~I neuron model, and the FHN model is a type~II neuron model (though LIF neurons are not prototypical type~I neurons \citep{Mato2008}, they are qualitatively more similar to this category). The difference between type~I and type~II neuron models is that type~I models undergo a saddle-node bifurcation as the input current (the bifurcation parameter) increases, whereas type~II models undergo a subcritical Hopf bifurcation \citep{Mato2008}. Qualitatively, this means that type~I models have continuous tuning curves, \ie they can have arbitrarily small firing rates, whereas type~II models have a discontinuity at their firing threshold, and cannot fire slower than a given rate. The fact that the LIF model is a type~I neuron model, in addition to other factors, makes it a better model of cortical neurons \citep{Koch1999}. The LIF model is therefore more relevant when examining population encoding as proposed to occur in cortex, though other neuron models may be preferable for certain sensory coding situations. Previous numerical experiments that have examined stochastic resonance and heterogeneity together, such as \cite{Stocks2000,McDonnell2006}, have used binary threshold units in their simulations, which are qualitatively very similar to type~II neurons (but without the temporal component); the results of these experiments may have limited applicability to cortical neural systems.

The heterogeneity of biases added to neuronal populations had a much more beneficial effect on information transmission in LIF neurons than in FHN neurons (\fig{info}). We hypothesize that this is because of key differences in the shape of the neuron tuning curves, combined with the fact that heterogeneity was only added to one neuron parameter, namely the bias voltage $b_i$. The FHN model, being a type~II neuron model, has a tuning curve that is qualitatively similar to a binary threshold unit, with a discontinuous increase in firing rate at the firing threshold, and offering little change in firing rate for signals above the threshold (\fig{tuning}A). On the other hand, the LIF model, being a type~I neuron model, has a continuous and monotonically increasing tuning curve, capable of having arbitrarily small firing rates, and producing significantly higher firing rates for input signals that are well above the firing threshold than for those slightly above the firing threshold (\fig{tuning}B). If a high-frequency signal crosses the firing thresholds of many FHN neurons in quick succession, all these neurons will begin firing both at approximately the same time and with approximately the same rate, taking a long time to desynchronize. With LIF neurons, the high-frequency signal can still cross the firing thresholds of many neurons simultaneously, causing them to begin firing at approximately the same time. However, the signal will result in significantly higher firing rates for neurons with lower firing thresholds than for those with higher firing thresholds, allowing the group of neurons to desynchronize quickly. This key difference between the tuning curves of FHN and LIF neurons results in better desynchronization for LIF neurons when heterogeneity is only added to the bias voltage. We hypothesize that if heterogeneity were added to other neuron parameters, for example giving different FHN neurons different maximum firing rates, then FHN neurons would be able to desynchronize as quickly as LIF neurons and heterogeneity would offer a similar benefit in information transmission to stochastic resonance in FHN neurons. A system with this level of heterogeneity would require a more complex decoding system than the simple summing neuron model used in this paper, perhaps using the tuning curves to solve for the optimal linear decoders as described by \cite{Eliasmith2003}, and we leave this for future work.

Noise and heterogeneity have a fundamental similarity which goes deeper than their mutual ability to increase information transmission in neuronal populations: noise and heterogeneity are both a form of random variation. Specifically, noise corresponds to variation within the phase space (state space) of a system over time, and heterogeneity corresponds to variation within the parameter space of a system across a population. It is important to emphasize that the heterogeneity used in this paper was completely random; the heterogeneous biases did not have to be specifically chosen or fine-tuned in any way, indicating that random heterogeneity could be implemented in a messy biological system like the brain. Finally, in analogy to stochastic resonance with noise, there is a critical level of heterogeneity that optimizes information transmission (\fig{infohetero}). Thus noise and heterogeneity should be thought of as analogous mechanisms, but in different parts of the neuronal model (the state space versus the parameter space, respectively).

\begin{figure}
  \ifx\hidefigures\undefined
    \centering
    \includegraphics{../figures/bw/figure6_infohetero.eps}
  \fi
  \caption{
    \textbf{Heterogeneity exhibits resonance effects in information transfer.} In an experiment similar to that shown in \fig{info}, the mutual information was measured between the input signal $s(t)$ and output signal $r(t)$ of populations of 64 LIF neurons with varying levels of noise and heterogeneity. The key difference is that in this figure, the radius of heterogeneity $b_r$ (where $b_i \in [-b_r, b_r]$) is varied along the $x$-axis, and each individual trace depicts a different noise level $\sigma_\eta$. At lower levels of noise ($\sigma_\eta = 1.0 \times 10^{-6}, 3.0 \times 10^{-3}$), there is a critical level of heterogeneity that optimizes information transmission. Specifically, this level of heterogeneity is between one and two times the standard deviation of the input signal $\sigma_s = 0.1$. At the optimal level of noise ($\sigma_\eta = 1.0 \times 10^{-2}$, as shown in \fig{info}), heterogeneity has no additional benefit: the noise itself is enough to allow for good information transmission. Finally, at high levels of noise ($\sigma_\eta = 1.0 \times 10^{-1}$), the noise overpowers the input signal, and no amount of heterogeneity can improve information transmission. This same experiment was performed with FHN neurons, but the change in information transmission due to heterogeneity was minimal, likely because the FHN tuning curves have little variation above the input stimulus level (see \scn{discussion}).
  }
  \figlabel{infohetero}
\end{figure}

The extent to which actual biological neural systems exploit either heterogeneity or stochastic resonance for improved information encoding and transmission remains unclear, and may vary significantly from system to system. One recent study shows that heterogeneity can be very important in encoding sensory information for certain electric fishes \citep{Marsat2010}. Another recent study shows SR effects in human auditory cortex, specifically that noise can increase correlation both within and between some cortical regions \citep{Ward2010}. More studies of this nature are required to determine both what neural systems show evidence of heterogeneity and stochastic resonance, and more importantly, whether these systems actually take advantage of heterogeneity and stochastic resonance for improved population coding. Our contribution is demonstrating \emph{how} a neural system could exploit either heterogeneity or stochastic resonance, or both, for improved information processing characteristics.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Models and Methods}
\scnlabel{methods}

Our model consists of an input signal $s(t)$ encoded by a population of neurons, specifically having the same signal injected into the soma of all cells in the population (see \eqn{fhn} and \eqn{lif}). We ran simulations using both the FitzHugh-Nagumo (FHN) neuron model and the leaky integrate-and-fire (LIF) neuron model. Signal decoding is modeled by summing the spikes of each neuron in the population and low-pass filtering the result to obtain the output signal $r(t)$. This can be thought of as analogous to determining the somatic current of a single receiving neuron, where the filtering is performed by synapses. The input and output signals are then compared using mutual information, to measure how well the output signal encodes the input signal. Independent and identically distributed (i.i.d.) additive Gaussian noise processes are added to each neuron, with standard deviation $\sigma_\eta$ used to modulate the noise level in each experiment. Heterogeneity is also added by choosing neuron biases $b_i$ from a random uniform distribution, where the width of the distribution controls the degree of heterogeneity.

\textbf{FitzHugh-Nagumo (FHN) model.} The FHN model is a simplification of the Hodgkin-Huxley model \citep{FitzHugh1961}. We chose to use the standard formulation presented by \cite{Izhikevich2006}, given for the $i$-th neuron by two coupled ordinary differential equations:
\begin{equation}
  \begin{array}{rl}
    \dot{v_i} & = v_i - \frac{1}{3}v_i^3 - w_i + \beta_i + e_i s(t) + \eta_i(t) \vspace{1em}\\
    \dot{w_i} & = 0.08(v_i - 0.8w_i + 0.7),
  \end{array}
  \eqnlabel{fhn}
\end{equation}
where $v_i(t)$ is a fast (voltage) variable, $w_i(t)$ is a slow (recovery) variable, $\beta_i$ is a constant bias voltage, $e_i$ is a simple encoder equal to 1 for ``on'' neurons and $-1$ for ``off'' neurons, $s(t)$ is an input signal common to all neurons, and $\eta_i(t)$ is a Gaussian white noise process with autocorrelation $\left<\eta_i(t)\eta_j(t + \tau)\right> = \sigma^2 \delta_{ij}\delta(\tau)$. We chose $\beta_i = 0.3216 - b_i$, with the offset 0.3216 chosen such that the bias $b_i$ corresponds to the threshold at which the neuron begins firing.

\textbf{Leaky integrate-and-fire (LIF) model.} The LIF model describes a neuron as a membrane with resistance $R$ and capacitance $C$, which fires spikes when the membrane voltage $v(t)$ crosses the threshold $V_{th}$. It is governed by the differential equation
\begin{equation}
  \tau_{RC} \dot{v_i} = -v_i + \beta_i + \alpha \left( e_i s(t) + \eta_i(t) \right),
  \eqnlabel{lif}
\end{equation}
where $v_i(t)$ is the membrane voltage, $\tau_{RC} = RC$ is the membrane time constant, $\alpha$ is the input signal gain, $\beta_i$ is a constant bias voltage, $\eta_i(t)$ is a delta-correlated Gaussian white noise process (as with the FHN neuron model), and $s(t)$ is an input signal common to all neurons. When a neuron voltage reaches the threshold voltage $V_{th} = 1$, the neuron fires a spike, and begins a refractory period. The length of the refractory period is given by the refractory time constant $\tau_{ref}$, and during this time the neuron voltage is held at zero. Once the refractory period is finished, the neuron obeys \eqn{lif} until another spike occurs.

We chose $\tau_{RC}$ to be 20 ms, which is typical for cortical neurons \citep{McCormick1985}. We then chose $\tau_{ref}$ to be 33 ms, thus giving a maximum firing rate (30 Hz) approximately the same as the FHN model. We chose $\alpha$ to be 15, to achieve a similar maximum firing rate to the FHN model over the range of inputs used. We chose $\beta_i = V_{th} - \alpha b_i$, such that $b_i$ is a bias that corresponds to the threshold at which the neuron begins firing.

\textbf{Input signal generation.} For the input signal $s(t)$, we used an aperiodic random signal with zero mean and equal power below a nominal maximum frequency. The signal had a standard deviation of $\sigma_s = 0.1$. It had a maximum frequency of 5Hz for the simulations, roughly corresponding to one fifth of the maximum firing frequency of the neurons over the range of inputs.

\textbf{Output signal decoding.} To determine the output signal $r(t)$, we used the basic model of a summing neuron similar to the one proposed by \cite{Stocks2001}. Specifically, the output signal $r$ is given by
\begin{equation}
  \begin{array}{rcl}
    \tau \dot{r} & = & -r + \sum\limits_{i=1}^N q_i \vspace{0.5em}\\
    q_i & = & \begin{cases} v_i & \text{if } v_i > V_{th} \\ 0 & \text{otherwise} \end{cases},
  \end{array}
\end{equation}
where $N$ is the number of neurons in the encoding population, and $V_{th}$ is the firing threshold ($V_{th} = 0$ for the FHN model, and $V_{th} = 2$ for the LIF model). This model simulates a summing neuron which only receives the spikes (\ie parts of the neuron voltage $v_i$ greater than the threshold $V_{th}$, and then low-pass filters the sum of these spikes, a basic description of how a summing neuron membrane sums and filters input voltages. 

\textbf{Mutual information.} We use Shannon mutual-information as the measure of similarity between the input and output signal \citep{Heneghan1996,Stocks2001}, where the mutual information $I$ (in bits) between the output signal $r$ and input signal $s$ is given by
\begin{equation}
  I(s,r) = \sum\limits_{s \in S} \sum\limits_{r \in R} p(s,r) \log_2\left(\frac{p(s,r)}{p(s)p(r)}\right),
\end{equation}
where $S$ is the domain of the input signal $s(t)$ and $R$ is the domain of the output signal $r(t)$. To compute the joint probability distribution $p(s,r)$ for discrete signals $s(t)$ and $r(t)$, the domains $S$ and $R$ are each divided into equal bins, and a joint histogram of the points $[s(t_i),r(t_i)]$ is formed. This histogram is used to approximate the joint probability function $p(s,r)$ by scaling it so the sum across all $s$ and $r$ is unity (\ie dividing the histogram by the total number of points $N$). For all simulations, we divided the domains $S$ and $R$ into 19 bins each.

One shortfall of this comparison method is that it does not account for the delay between the input and output signals, due to the non-zero reaction time of the neurons. In neural systems, some delay is to be expected, and we should not penalize our simulated networks for not being able to encode and decode an input signal instantaneously. The difficulty in comparing a delayed version of the input signal with the output signal is that it raises the question of what length of delay should be used. Each pair of noise and heterogeneity levels may conceivably have a unique optimal delay, and in order to not favor either noise or heterogeneity, the optimal delay (\ie the delay that produces the highest mutual information) should be calculated and used at each pair of noise and heterogeneity levels. In the interest of simplicity, we decided to instead compare the input and output signals with no delay, as is consistent with previous experiments \citep{Stocks2001}, and allows the benefits of noise and heterogeneity on response time to be measured.

\textbf{Heterogeneity.} To introduce heterogeneity into the population, we varied the bias voltages of the neurons. In homogeneous populations, all neurons had bias voltages equal to zero, the mean of the input signal. Heterogeneous populations had neurons with uniformly randomly distributed biases chosen from a range centered around the mean. In particular, three ranges were used: $[-0.01,0.01]$, $[-0.05,0.05]$, and $[-0.2,0.2]$, corresponding to small, medium and large levels of heterogeneity. The ranges were chosen to be roughly logarithmically distributed, with the largest range corresponding to biases that covered two standard deviations of the input signal.

\textbf{``On'' and ``off'' neurons.} To help minimize subthreshold stochastic resonance effects from significantly contributing to our results, we used populations that contained an equal number of ``on'' and ``off'' neurons, \ie neurons that both increase firing rates as the input signal increases, and increase firing rates as the signal decreases. In terms of our model, this means that half the neurons of each population had $e_i = 1$, and the other half had $e_i = -1$. This means that half the population represents the positive and negative parts of the signal, respectively, and only a small amount of information is gained by the subthreshold SR effect, which allows each half of the population to represent both positive and negative signal components.

\textbf{Population phase.} To determine how synchronous a population of neurons was, we measured the standard deviation of the neuron phases around the mean population phase. FHN neuron phases were determined by measuring the angle of the $(V(t),W(t))$ point in phase space around a central point, chosen to be $(-0.22,0.60)$ since this lies roughly in the center of the FHN limit cycle. For LIF neurons, the refractory period was scaled to correspond to the first $180^\circ$ of phase, and if the neuron was out of the refractory period, the phase was computed as $180^\circ ( 1 + \max(0,\min(1,V(t))) )$. The mean phase $\bar\theta$ was computed as
\begin{equation}
  \bar\theta(t) = \tan^{-1} \left( \frac{1}{N}\sum\limits_{i=1}^N \cos\theta_i(t), 
                                  \frac{1}{N}\sum\limits_{i=1}^N \sin\theta_i(t) \right),
\end{equation}
where $\theta_i(t)$ is the phase of neuron $i$ at time $t$, $N$ is the number of neurons in the population, and $\alpha = \tan^{-1}(x,y)$ is the two-argument arctangent function such that $\tan\alpha = y/x$. The standard deviation $\theta_\sigma(t)$ of the population phase is then given by
\begin{equation}
  \theta_\sigma(t) = \sqrt{ \frac{1}{N} \sum\limits_{i=1}^N (\mathrm{wrap}(\theta_i - \bar\theta))^2 },
\end{equation}
where the function $\mathrm{wrap}(\alpha) = (\alpha + \pi \mod 2\pi) - \pi$ converts the angle $\alpha$ to its equivalent angle in the range $[-\pi,\pi)$. \fig{phase} presents the mean over all simulation time of the standard deviation of the phase.

\textbf{Simulation details.} The information transmission (\fig{info}) and phase (\fig{phase}) plots were computed using noise levels in the range $\sigma_\eta \in [10^{-9},1]$ for FHN neurons and in the range $\sigma_\eta \in [10^{-6},1]$ for LIF neurons, with the levels logarithmically spaced in steps of $10^{0.1}$. We chose the low end of these ranges to be an insignificant level of noise (\ie with information transmission qualitatively the same as the no-noise case) and increasing until the noise completely overpowered the signal (\ie only a negligible amount of signal information could be recovered). At each noise level, and each of the four levels of heterogeneity, 25 simulations were performed, each having a simulation time of 4.5 seconds, with the first 0.5 seconds of each simulation being discarded to allow any transients created by the initial conditions to settle. The time step used varied depending on the noise level: larger levels of noise are more unstable and require a smaller time step, so a step of $\Delta t = 10^{-5}$ was used for $\sigma_\eta >= 0.1$, $\Delta t = 2.5 \times 10^{-5}$ for $\sigma_\eta >= 0.01$, and $\Delta t = 10^{-4}$ for $\sigma_\eta < 0.01$. For each simulation, new sets of both random biases $b_i$ and noise $\eta_i(t)$ were generated for each neuron. The final traces in the figures show the mean over all 25 simulations. The standard error of the mean at each point was not plotted because it was small in all cases, with statistically significant differences between the traces.

The step response plots (\fig{step}) represent the average population step response of a $N = 100$ neuron population over 100 trials. Each trial consists of presenting a population of $N = 100$ neurons with a step input $S_{step}(t)$ of 2 second duration, where the step input is equal to $-0.01$ for $t < 1.0$ and $0.01$ for $t >= 1.0$, with a time step $\Delta t = 10^{-4}$. The time shown in the plots corresponds to simulation time $t \in [0.9, 1.4]$, that is 0.1 seconds before the leading edge of the step to 0.4 seconds afterward. Output responses are scaled so that the average response for simulation time $t > 1.5$ corresponds to $0.01$, the final value of the step function.

The tuning curve plots (\fig{tuning}) were all calculated empirically by measuring the firing rate of simulated neurons. The range of inputs measured was $[-0.2, 0.2]$, divided into 51 equally spaced points (intervals of $0.008$). At each point, a constant signal at that input value was presented to $N = 30$ neurons, with a duration of 5.5 seconds. The first 0.5 seconds of neuron output was discarded, to reduce the effects of initial condition transients, and the average spike rate for each neuron over the remaining time period was calculated. FHN neurons were considered to spike when they maintained a voltage level $V(t) > 0$ for at least 0.01 seconds, and LIF neurons were considered to spike whenever the voltage crossed the threshold $V_{th}$, as given by the neuron model. The tuning curves presented in the plots show the average firing rate of all $N = 30$ neurons for each input value.

\subsection*{Acknowledgments}

We thank Jeff Orchard for feedback on an earlier draft of this manuscript.

\begin{thebibliography}{100}
\providecommand{\natexlab}[1]{#1}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi:\discretionary{}{}{}#1}\else
  \providecommand{\doi}{doi:\discretionary{}{}{}\begingroup
  \urlstyle{rm}\Url}\fi

\bibitem[Chialvo et~al., 1997]{Chialvo1997}
Chialvo, D., Longtin, A., and M\"{u}ller-Gerking, J. (1997).
\newblock {Stochastic resonance in models of neuronal ensembles}.
\newblock {\em Physical review E}, 55(2):1798--1808.

\bibitem[Collins et~al., 1995]{Collins1995}
Collins, J.~J., Chow, C.~C., and Imhoff, T.~T. (1995).
\newblock {Stochastic resonance without tuning}.
\newblock {\em Nature}, 376:236--238.

\bibitem[Eliasmith and Anderson, 2003]{Eliasmith2003}
Eliasmith, C. and Anderson, C.~H. (2003).
\newblock {\em {Neural Engineering: Computation, Representation, and Dynamics
  in Neurobiological Systems}}.
\newblock MIT Press, Cambridge, MA.

\bibitem[Fitzhugh, 1961]{FitzHugh1961}
Fitzhugh, R. (1961).
\newblock {Impulses and physiological states in theoretical models of nerve
  membrane}.
\newblock {\em Biophysical journal}, 1(1948):445--466.

\bibitem[Gammaitoni et~al., 1998]{Gammaitoni1998}
Gammaitoni, L., H\"{a}nggi, P., Jung, P., and Marchesoni, F. (1998).
\newblock {Stochastic resonance}.
\newblock {\em Reviews of Modern Physics}, 70(1):223--287.

\bibitem[Heneghan et~al., 1996]{Heneghan1996}
Heneghan, C., Chow, C., Collins, J.~J., Imhoff, T.~T., Lowen, S.~B., and Teich,
  M.~C. (1996).
\newblock {Information measures quantifying aperiodic stochastic resonance.}
\newblock {\em Physical Review E}, 54(3):R2228--R2231.

\bibitem[Izhikevich and FitzHugh, 2006]{Izhikevich2006}
Izhikevich, E.~M. and FitzHugh, R. (2006).
\newblock {FitzHugh-Nagumo model}.
\newblock {\em Scholarpedia}, 1(9):1349.

\bibitem[Koch, 1999]{Koch1999}
Koch, C. (1999).
\newblock {\em {Biophysics of computation: Information processing in single
  neurons}}.
\newblock Oxford University Press, New York, NY.

\bibitem[Longtin and Chialvo, 1998]{Longtin1998}
Longtin, A. and Chialvo, D.~R. (1998).
\newblock {Stochastic and Deterministic Resonances for Excitable Systems}.
\newblock {\em Physical Review Letters}, 81(18):4012--4015.

\bibitem[Machens et~al., 2010]{Machens2010}
Machens, C.~K., Romo, R., and Brody, C.~D. (2010).
\newblock {Functional, but not anatomical, separation of "what" and "when" in
  prefrontal cortex.}
\newblock {\em The Journal of neuroscience : the official journal of the
  Society for Neuroscience}, 30(1):350--60.

\bibitem[Marsat and Maler, 2010]{Marsat2010}
Marsat, G. and Maler, L. (2010).
\newblock {Neural heterogeneity and efficient population codes for
  communication signals.}
\newblock {\em Journal of neurophysiology}, 104(5):2543--55.

\bibitem[Mato and Samengo, 2008]{Mato2008}
Mato, G. and Samengo, I. (2008).
\newblock {Type I and type II neuron models are selectively driven by
  differential stimulus features}.
\newblock {\em Neural computation}, 2440:2418--2440.

\bibitem[McCormick et~al., 1985]{McCormick1985}
McCormick, D.~A., Connors, B.~W., Lighthall, J.~W., and Prince, D.~A. (1985).
\newblock {Comparative electrophysiology of pyramidal and sparsely spiny
  stellate neurons of the neocortex}.
\newblock {\em Journal of Neurophysiology}, 54:782--806.

\bibitem[McDonnell and Abbott, 2009]{McDonnell2009}
McDonnell, M.~D. and Abbott, D. (2009).
\newblock {What is stochastic resonance? Definitions, misconceptions, debates,
  and its relevance to biology.}
\newblock {\em PLoS Computational Biology}, 5(5):e1000348.

\bibitem[McDonnell et~al., 2006]{McDonnell2006}
McDonnell, M.~D., Stocks, N.~G., Pearce, C.~E., and Abbott, D. (2006).
\newblock {Optimal information transmission in nonlinear arrays through
  suprathreshold stochastic resonance}.
\newblock {\em Physics Letters A}, 352(3):183--189.

\bibitem[McDonnell and Ward, 2011]{McDonnell2011}
McDonnell, M.~D. and Ward, L.~M. (2011).
\newblock {The benefits of noise in neural systems: bridging theory and
  experiment}.
\newblock {\em Nature Reviews Neuroscience}, 12(7):415--426.

\bibitem[Stocks, 2000]{Stocks2000}
Stocks, N.~G. (2000).
\newblock {Suprathreshold stochastic resonance in multilevel threshold
  systems}.
\newblock {\em Physical review letters}, 84(11):2310--3.

\bibitem[Stocks, 2001]{Stocks2001a}
Stocks, N.~G. (2001).
\newblock {Information transmission in parallel threshold arrays:
  Suprathreshold stochastic resonance}.
\newblock {\em Physical Review E}, 63(4):1--9.

\bibitem[Stocks and Mannella, 2001]{Stocks2001}
Stocks, N.~G. and Mannella, R. (2001).
\newblock {Generic noise-enhanced coding in neuronal arrays}.
\newblock {\em Physical Review E}, 64:030902.

\bibitem[Stocks et~al., 1996]{Stocks1996}
Stocks, N.~G., Stein, N., Short, H., Mannella, R., Luchinsky, D., McClintock,
  P., and Dykman, M. (1996).
\newblock {Noise-induced linearization and delinearization}.
\newblock In {\em Fluctuations and Order}, pages 53--67.

\bibitem[Tomita et~al., 1974]{Tomita1974}
Tomita, K., Ohta, T., and Tomita, H. (1974).
\newblock {Irreversible circulation and orbital revolution}.
\newblock {\em Prog. Theor. Phys}, 52(6):1744--1765.

\bibitem[Ward et~al., 2010]{Ward2010}
Ward, L.~M., MacLean, S.~E., and Kirschner, A. (2010).
\newblock {Stochastic Resonance Modulates Neural Synchronization within and
  between Cortical Sources}.
\newblock {\em PLoS ONE}, 5(12):e14371.

\bibitem[Wiesenfeld et~al., 1994]{Wiesenfeld1994}
Wiesenfeld, K., Pierson, D., and Pantazelou, E. (1994).
\newblock {Stochastic resonance on a circle}.
\newblock {\em Physical Review Letters}, 72(14):2125--2129.

%% \bibitem[{LastName(2009)}]{Ref2009}
%% LastName, A. (2009).
%% \newblock Title for the first reference.
%% \newblock \emph{Journal of the first reference}, \emph{3}, 18 -- 88.

%% \bibitem[{Authors et~al.(2008)Author1, Author2, \&
%%   Author3}]{Ref2008}
%% Author1, A., Author2, A., \& Author3, A. (2008).
%% \newblock Title for the second reference.
%% \newblock \emph{Journal for the second reference}, \emph{5}, 188 -- 200.

\end{thebibliography}

\end{document}
